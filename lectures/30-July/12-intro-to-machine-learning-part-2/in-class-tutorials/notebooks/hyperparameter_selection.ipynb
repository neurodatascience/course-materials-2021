{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.base import clone\n",
    "from scipy.linalg import svd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dof(X, alpha_range):\n",
    "    U, S, Vt = svd(X)\n",
    "    return np.sum(S ** 2 / (S ** 2 + alpha_range[:, None]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = np.logspace(-3, 3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RidgeCV(alphas=alpha_range, store_cv_values=True)\n",
    "X, y, true_coef = make_regression(\n",
    "    noise=5,\n",
    "    effective_rank=2,\n",
    "    random_state=0,\n",
    "    coef=True,\n",
    "    shuffle=False,\n",
    "    n_samples=100,\n",
    ")\n",
    "train = np.arange(len(y) // 2)\n",
    "test = np.arange(len(y) // 2, len(y))\n",
    "print(true_coef[:20])\n",
    "# fitted = clone(model).fit(X, y)\n",
    "# mse = fitted.cv_values_.mean(axis=0)\n",
    "# r2 = 1 - fitted.cv_values_.sum(axis=0) / (len(y) * y.var())\n",
    "coef = []\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "for alpha in alpha_range:\n",
    "    ridge = Ridge(alpha, fit_intercept=True).fit(X[train], y[train])\n",
    "    train_mse.append(mean_squared_error(y[train], ridge.predict(X[train])))\n",
    "    test_mse.append(mean_squared_error(y[test], ridge.predict(X[test])))\n",
    "    coef.append(ridge.coef_)\n",
    "coef = np.asarray(coef)\n",
    "fig, ax = plt.subplots(3, 1, sharex=True, figsize=(6, 8))\n",
    "# test_r2 = 1 - test_mse / y[test].var()\n",
    "ax[0].plot(alpha_range, test_mse)\n",
    "ax[0].plot(alpha_range, train_mse)\n",
    "ax[0].legend([\"testing error\", \"training error\"])\n",
    "ax[0].set_ylabel(\"Mean squared error\")\n",
    "ax[1].plot(alpha_range, test_mse)\n",
    "ax[1].set_ylabel(\"Mean squared error\")\n",
    "ax[1].legend([\"testing error\"], loc=\"lower right\")\n",
    "ax[2].plot(alpha_range, coef[:, 0:50:5])\n",
    "# ax.axhline(true_coef[1])\n",
    "ax[2].set_xscale(\"log\")\n",
    "ax[2].set_ylabel(\"Example coefficients\")\n",
    "ax[2].set_xlabel(\"Regularization hyperparameter ɑ\")\n",
    "ax[0].set_title(\n",
    "    \"Hyperparameter choice matters: Ridge regression\\n\"\n",
    "    \"MSE and coefficients for a range of ɑ\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.gcf().savefig(\"hyperparameter_selection.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
